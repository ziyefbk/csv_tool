# 进一步性能优化方案 - 7GB 大文件

## 当前已实现的优化

1. ✅ 内存映射 (mmap) - 避免将整个文件加载到内存
2. ✅ 稀疏行索引 - 每 N 行记录一次位置
3. ✅ LRU 页面缓存 - 缓存最近访问的页面
4. ✅ 索引持久化 - 保存到 .idx 文件，下次打开无需重建
5. ✅ SIMD 加速 - 使用 memchr 加速换行符查找
6. ✅ 多线程并行构建 - 使用 rayon 并行处理（>100MB）
7. ✅ **动态索引粒度** - 大文件自动使用更大粒度（已实现）
8. ✅ **快速预览** - 立即显示前 N 行，无需等待索引（已实现）

## 7GB 文件的瓶颈分析

对于 7GB 文件，第一次打开慢的原因：
- **索引构建**：需要扫描整个文件找到所有换行符
- **行数统计**：需要完整扫描才能知道总行数
- 即使并行处理，7GB 文件仍需要几秒到几十秒

## 进一步优化方案

### 方案 1: 渐进式加载（推荐）

**思路**：先快速显示数据，后台构建完整索引

```
用户体验：
1. 打开文件 -> 立即显示第一页（<100ms）
2. 后台构建索引
3. 索引完成后，分页功能可用
```

**实现**：
- 立即加载前 N 行（不需要完整索引）
- 后台线程构建完整索引
- 索引完成前禁用分页，显示"正在加载..."

### 方案 2: 增大索引粒度

**当前**：每 1000 行记录一次
**优化**：对于大文件，增大到 10000 或更大

```rust
let granularity = if file_size > 1_000_000_000 { // 1GB
    10_000
} else if file_size > 100_000_000 { // 100MB
    5_000
} else {
    1_000
};
```

**效果**：减少索引大小和构建时间 5-10 倍

### 方案 3: 采样估算行数

**思路**：不扫描全部文件，通过采样估算总行数

```rust
// 采样文件的前 10MB
let sample_size = min(file_size, 10 * 1024 * 1024);
let sample_lines = count_lines(&mmap[..sample_size]);
let avg_line_size = sample_size / sample_lines;
let estimated_total_rows = file_size / avg_line_size;
```

**效果**：毫秒级获得估算行数，用于显示进度

### 方案 4: 懒加载索引

**思路**：只在需要时构建索引

```
- 打开文件：不构建索引
- 用户翻页：按需构建当前页的索引区域
- 搜索功能：触发完整索引构建
```

**效果**：首次打开几乎瞬间完成

### 方案 5: 更高效的并行策略

**当前**：按 CPU 核心数分块
**优化**：
- 增大块大小（当前可能太小）
- 使用内存预取提示
- 减少同步开销

```rust
// 更大的块大小，减少线程调度开销
const CHUNK_SIZE: usize = 256 * 1024 * 1024; // 256MB per chunk
```

### 方案 6: 索引压缩

**思路**：使用差分编码压缩索引

```rust
// 原始索引：[0, 1000, 2000, 3000, ...]
// 差分编码：[0, 1000, 1000, 1000, ...]  // 更适合压缩
```

**效果**：减少索引文件大小和加载时间

## 推荐实施顺序

1. **立即实施**：方案 2（增大索引粒度）- 简单有效
2. **短期实施**：方案 1（渐进式加载）- 最佳用户体验
3. **中期实施**：方案 4（懒加载索引）- 复杂但高效
4. **长期优化**：方案 5 + 6 - 极限优化

## 预期效果

| 优化方案 | 7GB 文件首次打开时间 | 复杂度 |
|---------|-------------------|--------|
| 当前实现 | 10-30 秒 | - |
| 方案 2（增大粒度） | 3-10 秒 | 低 |
| 方案 1（渐进式） | <1 秒（显示首页） | 中 |
| 方案 4（懒加载） | <100ms | 高 |
| 全部优化 | <100ms | 高 |

## 已实现的优化

### 1. 动态索引粒度（已实现）

根据文件大小自动调整索引粒度：

```rust
// tauri/src/main.rs
let granularity = if file_size > 5_000_000_000 {      // > 5GB
    50_000  // 超大文件：每 50,000 行记录一次
} else if file_size > 1_000_000_000 {  // > 1GB
    20_000  // 大文件：每 20,000 行记录一次
} else if file_size > 100_000_000 {    // > 100MB
    5_000   // 中等文件：每 5,000 行记录一次
} else {
    1_000   // 小文件：每 1,000 行记录一次
};
```

**效果**：7GB 文件的索引构建时间减少约 20-50 倍

### 2. 快速预览功能（已实现）

新增 `quick_preview` API，允许立即显示数据：

```typescript
// 前端调用
const preview = await csvApi.quickPreview(filePath, 100);
// 立即显示前 100 行，不等待索引
```

**用户体验**：
1. 打开文件 → 立即显示前 100 行（< 100ms）
2. 显示"正在加载完整数据..."
3. 后台构建索引
4. 索引完成后，所有功能可用

**效果**：7GB 文件首次打开体验从 10-30 秒变为 < 1 秒

## 使用建议

### GUI 推荐流程

```typescript
async function openLargeFile(filePath: string) {
  // 1. 立即显示预览
  const preview = await csvApi.quickPreview(filePath, 100);
  displayData(preview.headers, preview.rows);
  showMessage(`预计 ${preview.estimated_rows.toLocaleString()} 行，正在加载...`);
  
  // 2. 后台打开完整文件（构建索引）
  const fileInfo = await csvApi.openFile(filePath);
  showMessage(`加载完成：${fileInfo.total_rows.toLocaleString()} 行`);
}
```

### CLI 推荐方式

对于 CLI，仍然需要等待索引构建。但第二次打开同一文件会非常快（使用缓存的索引）。

```bash
# 第一次打开（会构建索引）
csv-tool view large.csv -p 0 --page-size 100

# 第二次打开（使用缓存索引，瞬间完成）
csv-tool view large.csv -p 1000 --page-size 100
```

